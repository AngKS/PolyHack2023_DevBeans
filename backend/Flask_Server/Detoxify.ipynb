{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detoxify import Detoxify\n",
    "import pandas as pd\n",
    "\n",
    "# each model takes in either a string or a list of strings\n",
    "\n",
    "results = Detoxify('original').predict('example text')\n",
    "\n",
    "results = Detoxify('unbiased').predict(['example text 1','example text 2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxicity': 0.00064783124,\n",
       " 'severe_toxicity': 0.0001209842,\n",
       " 'obscene': 0.00018694326,\n",
       " 'threat': 0.000116240895,\n",
       " 'insult': 0.0001811189,\n",
       " 'identity_attack': 0.00014001914}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Detoxify('original').predict('example text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxicity': [0.00041021101060323417, 0.0004120281373616308],\n",
       " 'severe_toxicity': [1.5820104408703628e-06, 1.5051891750772484e-06],\n",
       " 'obscene': [2.8287167879170738e-05, 2.6851148504647426e-05],\n",
       " 'identity_attack': [7.005838415352628e-05, 7.465355156455189e-05],\n",
       " 'insult': [8.426110434811562e-05, 8.405766129726544e-05],\n",
       " 'threat': [2.3083515770849772e-05, 2.2790431103203446e-05],\n",
       " 'sexual_explicit': [1.4899213056196459e-05, 1.3704860975849442e-05]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Detoxify('unbiased').predict(['example text 1','example text 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = 'unitary/unbiased-toxic-roberta' # change this to your model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I think this problem should be solved now! there is no reason to keep it open any longer. I do not agree with your reasoning.'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Apply threshold\n",
    "threshold = 0.5\n",
    "predictions = (probabilities > threshold).int()\n",
    "\n",
    "# Get labels\n",
    "predicted_labels = [model.config.id2label[i] for i, predicted in enumerate(predictions[0]) if predicted == 1]\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RobertaForSequenceClassification(\n",
       "   (roberta): RobertaModel(\n",
       "     (embeddings): RobertaEmbeddings(\n",
       "       (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "       (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "       (token_type_embeddings): Embedding(1, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): RobertaEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x RobertaLayer(\n",
       "           (attention): RobertaAttention(\n",
       "             (self): RobertaSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): RobertaSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): RobertaIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): RobertaOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (classifier): RobertaClassificationHead(\n",
       "     (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (out_proj): Linear(in_features=768, out_features=16, bias=True)\n",
       "   )\n",
       " ),\n",
       " {0: 'toxicity',\n",
       "  1: 'severe_toxicity',\n",
       "  2: 'obscene',\n",
       "  3: 'identity_attack',\n",
       "  4: 'insult',\n",
       "  5: 'threat',\n",
       "  6: 'sexual_explicit',\n",
       "  7: 'male',\n",
       "  8: 'female',\n",
       "  9: 'homosexual_gay_or_lesbian',\n",
       "  10: 'christian',\n",
       "  11: 'jewish',\n",
       "  12: 'muslim',\n",
       "  13: 'black',\n",
       "  14: 'white',\n",
       "  15: 'psychiatric_or_mental_illness'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'toxicity', 'score': 0.6187502145767212}, {'label': 'obscene', 'score': 0.3156442940235138}, {'label': 'insult', 'score': 0.06359092146158218}, {'label': 'sexual_explicit', 'score': 0.0010141676757484674}, {'label': 'identity_attack', 'score': 0.00018176126468461007}, {'label': 'male', 'score': 0.00016716725076548755}, {'label': 'threat', 'score': 0.00015431440260726959}, {'label': 'jewish', 'score': 0.00011388254642952234}, {'label': 'homosexual_gay_or_lesbian', 'score': 8.736200106795877e-05}, {'label': 'black', 'score': 8.387407433474436e-05}, {'label': 'psychiatric_or_mental_illness', 'score': 6.513501284644008e-05}, {'label': 'severe_toxicity', 'score': 5.325994425220415e-05}, {'label': 'female', 'score': 3.6531579098664224e-05}, {'label': 'christian', 'score': 2.74693902611034e-05}, {'label': 'white', 'score': 2.1288897187332623e-05}, {'label': 'muslim', 'score': 8.408659596170764e-06}]]\n"
     ]
    }
   ],
   "source": [
    "LABELS = model.config.id2label\n",
    "\n",
    "# Create a list of dictionaries, each containing a label and its corresponding score\n",
    "results = [{'label': LABELS[i], 'score': prob.item()} for i, prob in enumerate(probabilities[0])]\n",
    "\n",
    "# Sort the results by score in descending order\n",
    "results.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "# Wrap the results in an additional list to match the desired output\n",
    "results = [results]\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
